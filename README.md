# ğŸ¬ Oscar Movies ETL Toolkit

This project implements a small, self-contained ETL toolkit to clean, transform, and analyze Oscar-nominated movie data using **Python**, **Pandas**, and **DuckDB**.

The pipeline ingests raw JSON files, normalizes messy fields (budget and year), executes analytical SQL queries via DuckDB, and exports a curated dataset to CSV.

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Input JSON files
â”‚ â””â”€â”€ processed/ # Generated outputs
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ cleaning/ # Budget and year normalization
â”‚ â”œâ”€â”€ duckdb_utils/ # DuckDB connection and query execution
â”‚ â”œâ”€â”€ io/ # Readers and writers
â”‚ â”œâ”€â”€ pipeline/ # Pipeline orchestration
â”‚ â””â”€â”€ config.py # Shared configuration (currency rates)
â”œâ”€â”€ tests/ # Unit tests
â”œâ”€â”€ README.md
â”œâ”€â”€ EXPLAIN.md
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Setup

### 1. Create and activate a virtual environment

```bash
python -m venv venv
source venv/Scripts/activate   # Windows (Git Bash)
```
### 2. Install dependencies

```
pip install -r requirements.txt
```

â–¶ï¸ Run the Pipeline
```
PYTHONPATH=. python src/pipeline/run_pipeline.py
```

This will:
- Read raw movie data from data/raw/
- Clean and normalize budget and year fields
- Join datasets using a stable identifier
- Execute a DuckDB SQL query
- Write the final CSV to:
    - data/processed/oscar_winners_post_1955_min_15mm.csv (This is defined in config.py)

ğŸ§ª Run Tests
```
PYTHONPATH=. pytest
```
Unit tests validate:
- Budget normalization edge cases
- Year extraction logic
- DuckDB query execution

ğŸ›  Troubleshooting
- ModuleNotFoundError: Ensure PYTHONPATH=. is set
- Empty output file: Verify budget and year cleaning logic
- DuckDB errors: Ensure column names in SQL match the DataFrame schema

